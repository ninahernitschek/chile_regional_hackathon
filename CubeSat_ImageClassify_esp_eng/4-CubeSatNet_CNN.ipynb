{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ed05bb-fe72-45c8-aaf4-f067ee02838e",
   "metadata": {},
   "source": [
    "# Notebook 4: Classification using CNN / Clasificación mediante CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38388419-5a1e-458f-9a07-1179014901ba",
   "metadata": {},
   "source": [
    "**Only run it if you’re adopting or experimenting with this model, as it can take up to two hours to train.**\n",
    "\n",
    "In this notebook, we will use a CNN model to classify the images, following the approach used in the following [paper](https://ui.adsabs.harvard.edu/abs/2023SPIE12729E..0KC/abstract).\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    " \n",
    " \n",
    "**Sólo ejecútelo si está adoptando o experimentando con este modelo, ya que puede tardar hasta dos horas en entrenarse.**\n",
    "\n",
    "\n",
    "\n",
    "En este cuaderno, utilizaremos un modelo CNN para clasificar las imágenes, siguiendo el enfoque utilizado en el siguiente [artículo científico](https://ui.adsabs.harvard.edu/abs/2023SPIE12729E..0KC/abstract).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13095d08-82fa-49dc-8613-e8d4d1320555",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f39eb1-6f50-40c3-8468-226c0c8ee6b4",
   "metadata": {},
   "source": [
    "### Reading the data / Lectura de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f576c9-784f-49d2-bf5d-6978edeb89d4",
   "metadata": {},
   "source": [
    "First, we’ll load the saved image and label data from the NumPy files.\n",
    " <hr style=\"border:2px solid gray\">\n",
    " \n",
    "En primer lugar, cargaremos la imagen guardada y los datos de la etiqueta de los archivos NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a8dd1-52ed-4514-8cd1-a59980fab45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Importing NumPy for numerical operations and array handling\n",
    "\n",
    "# Load the images and labels back from the saved NumPy files\n",
    "train_images = np.load('train_images.npy')  # Load image training data\n",
    "train_labels = np.load('train_labels.npy')  # Load label training data\n",
    "\n",
    "print(\"Data loaded successfully from NumPy files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953fc9a-d986-47e1-8872-213f8518cdb7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a46294-7dfd-46c0-9146-d53220c58dbb",
   "metadata": {},
   "source": [
    "### Train CubeCatNet CNN model / Entrenar el modelo CNN CubeCatNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54646112-a70d-4764-93bc-7f0b44e267b1",
   "metadata": {},
   "source": [
    "We will define and train a Convolutional Neural Network (CNN) model that was defined in [link](https://ui.adsabs.harvard.edu/abs/2023SPIE12729E..0KC/abstract).\n",
    "\n",
    "**⚠️**: When running this model, or any other deep learning model, make sure it’s not being run in parallel with your other team members. Running multiple models simultaneously on shared resources like CPUs can severely impact performance, leading to slower training times and potential resource conflicts. Coordinate with your team to ensure efficient use of the available hardware.\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    " \n",
    "\n",
    "Definiremos y entrenaremos un modelo de Red Neuronal Convolucional (CNN) que se definió en [link](https://ui.adsabs.harvard.edu/abs/2023SPIE12729E..0KC/abstract).\n",
    " \n",
    "**⚠️**: Cuando ejecutes este modelo, o cualquier otro modelo de aprendizaje profundo, asegúrate de que no se ejecuta en paralelo con otros miembros de tu equipo. La ejecución simultánea de varios modelos en recursos compartidos como CPU puede afectar gravemente al rendimiento, lo que ralentiza los tiempos de entrenamiento y puede provocar conflictos de recursos. Coordínate con tu equipo para garantizar un uso eficiente del hardware disponible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7dbc5a-b183-4974-a3fd-7ce74c4fa66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential  # Importing Sequential to build the model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense  # Importing necessary layers for the CNN\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-hot encode the labels (assuming you have 5 classes)\n",
    "train_labels = to_categorical(train_labels, num_classes=5)\n",
    "\n",
    "# Define the CNN model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(512, 512, 3)),  # Convolutional layer + ReLU activation\n",
    "    MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    Conv2D(32, (3, 3), activation='relu'),  # Convolutional layer + ReLU activation\n",
    "    MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    Conv2D(64, (3, 3), activation='relu'),  # Convolutional layer + ReLU activation\n",
    "    MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    Conv2D(128, (3, 3), activation='relu'),  # Convolutional layer + ReLU activation\n",
    "    MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    GlobalAveragePooling2D(),  # Global average pooling layer\n",
    "    Dense(5, activation='softmax')  # Output layer with 5 neurons (one for each class) + Softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model with appropriate loss function, optimizer, and metrics\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model defined and compiled successfully.\")\n",
    "\n",
    "# Train the model on the training data\n",
    "history = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=10,  # Number of epochs\n",
    "    batch_size=64,  # Batch size\n",
    ")\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5cfc3-f2cd-4918-a85f-b3f1f7a9e505",
   "metadata": {},
   "source": [
    "##### **⚠️ Freeing up Space** / **⚠️ Liberar espacio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17f896-bbef-4caa-9b56-ba9d228dfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Since we will no longer need the original training data (train_images), we can remove it from memory\n",
    "del train_images, train_labels\n",
    "\n",
    "# Force garbage collection to free up memory\n",
    "gc.collect()\n",
    "\n",
    "print(\"train_images, and train_labels removed from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01be5fd-f911-410b-81e4-f0cd21f4cb4f",
   "metadata": {},
   "source": [
    "#### Saving the CNN model /  Guardar el modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c650f7-cf36-4108-ad8b-b6cb7fbff267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('cnn_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9d935-fda3-4ea3-8b33-4d5bd7cefc3a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc867bfa-7257-4523-a593-6ee21c8a4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now proceed to remove the testing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b406d3d2-e4f3-47b9-a117-b18a1b48e808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
