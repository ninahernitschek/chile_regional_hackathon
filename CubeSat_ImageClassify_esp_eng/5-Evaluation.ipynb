{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac66dbc-7ba9-4115-98f7-f2470fb4b844",
   "metadata": {},
   "source": [
    "# Notebook 5: Evaluation / Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94be4a-3bc6-4ee8-aee8-76ff5de0c030",
   "metadata": {},
   "source": [
    "In this notebook, we will compare the ML pipeline from notebook 3 with the Deep Learning pipeline from notebook 4.\n",
    " <hr style=\"border:2px solid gray\">\n",
    "\n",
    "En este Jupyter notebook, compararemos el proceso de aprendizaje automático del Notebook 3 con el proceso de aprendizaje profundo del Notebook 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec781abf-7ce3-4740-8a88-35e9aae2e2a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc76d53-6a3a-4508-9196-66e20ae4c16f",
   "metadata": {},
   "source": [
    "## Ensuring Fair Evaluation / Garantizar una evaluación justa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad12b9-e716-48c6-8bd9-382ba2f1349c",
   "metadata": {},
   "source": [
    "This code sets the CPU affinity to core 0, ensuring that all threads and processes are restricted to a single CPU core. By doing so, you ensure consistent performance evaluation, as it prevents interference from other cores and provides a controlled environment for comparing methods on the same core.\n",
    " <hr style=\"border:2px solid gray\">\n",
    "\n",
    "Este código establece la afinidad de la CPU en el núcleo 0, asegurando que todos los hilos y procesos están restringidos a un único núcleo de la CPU. De este modo, se garantiza una evaluación coherente del rendimiento, ya que se evitan las interferencias de otros núcleos y se proporciona un entorno controlado para comparar métodos en el mismo núcleo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbdaae3-0ce9-4121-aa07-9b0658f33712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "# Set CPU affinity to core 0 (the first core)\n",
    "p = psutil.Process(os.getpid())\n",
    "p.cpu_affinity([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5dc848-0b52-486c-815c-105cb134ae30",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cecd5e0-992a-442a-8147-b0c3231949be",
   "metadata": {},
   "source": [
    "## Pipeline Evaluation / Evaluación de la cadena de procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e78923-44b4-480c-95ae-4d950255d72e",
   "metadata": {},
   "source": [
    "In this step, we are developing an evaluation strategy that aims to provide a comprehensive assessment of a given machine-learning pipeline by combining both performance metrics and resource usage metrics. Here’s what we’re trying to achieve:\n",
    "\n",
    "1. **Performance Metrics Evaluation**:\n",
    "    - **Accuracy and F1 Score**: Measure the model’s ability to correctly classify images.\n",
    "\t- **Confusion Matrix Analysis**: Identify specific classes where the model may be underperforming.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "2. **Resource Utilization Assessment**:\n",
    "\t- **Evaluation Time**: Determine the total time taken for preprocessing and prediction.\n",
    "\t- **Pipeline Size**: Calculate the combined size of the trained model and preprocessing steps to understand storage requirements.\n",
    "\t- **Memory Consumption**: Monitor peak memory usage during evaluation to ensure it fits within hardware constraints.\n",
    "\t- **CPU Usage**: Measure average CPU utilization to evaluate computational efficiency.\n",
    "\n",
    "\n",
    "    The primary objective is to not only achieve high classification accuracy but also to assess and optimize the pipeline’s computational efficiency and resource utilization. This holistic approach ensures that the model is not just effective but also practical for deployment, especially in environments with limited computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c319b04-d065-43a4-87e6-d08f74963912",
   "metadata": {},
   "source": [
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "En este paso, estamos desarrollando una estrategia de evaluación que tiene como objetivo proporcionar una evaluación completa de una determinada canalización de aprendizaje automático mediante la combinación de métricas de rendimiento y métricas de uso de recursos. Esto es lo que intentamos conseguir:\n",
    "\n",
    "\n",
    "1. **Evaluación de las métricas de rendimiento**:\n",
    "    - **Precisión y puntuación F1**: Mide la capacidad del modelo para clasificar correctamente las imágenes.\n",
    "\t- **Análisis de la matriz de confusión**: Identificar clases específicas en las que el modelo puede estar rindiendo por debajo de lo esperado.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "2. **Evaluación de la utilización de recursos**:\n",
    "\t- **Tiempo de evaluación**: Determinar el tiempo total empleado en el preprocesamiento y la predicción.\n",
    "\t- **Tamaño del modelo**: Calcular el tamaño combinado del modelo entrenado y los pasos de preprocesamiento para comprender los requisitos de almacenamiento.\n",
    "\t- **Consumo de memoria**: Supervise el uso máximo de memoria durante la evaluación para asegurarse de que se ajusta a las limitaciones del hardware.\n",
    "\t- **Uso de la CPU**: Mida la utilización media de la CPU para evaluar la eficiencia computacional.\n",
    "\n",
    "\n",
    "El objetivo principal no es sólo lograr una alta precisión de clasificación, sino también evaluar y optimizar la eficiencia computacional y la utilización de recursos de la tubería. Este enfoque holístico garantiza que el modelo no solo sea eficaz, sino también práctico para su despliegue, especialmente en entornos con recursos computacionales limitados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45240736-9853-4e23-b1d4-4b2add8616a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.pre import evaluate_pipeline # A built-in function to evaluate a given ML pipeline by preprocessing, predicting, and calculating performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc0062-c0a5-4a45-9cc9-83f3318b8644",
   "metadata": {},
   "source": [
    "**Inputs:**\n",
    "- **model**: The trained machine learning model to evaluate.\n",
    "- **X_test_raw**: Raw test data that needs to be preprocessed before evaluation.\n",
    "- **y_test**: True labels corresponding to the test data for performance comparison.\n",
    "- **preprocessing_fn**: A function used to preprocess the raw test data.\n",
    "    \n",
    "**Outputs:**\n",
    "- **metrics**: A dictionary containing various evaluation metrics like accuracy, F1 score, evaluation time, memory usage, CPU usage, and pipeline size.\n",
    "\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Entradas:**\n",
    "- **model**: El modelo de aprendizaje automático entrenado para evaluar.\n",
    "- **X_test_raw**: Datos de prueba sin procesar que necesitan ser preprocesados antes de la evaluación.\n",
    "- **y_test**: Etiquetas verdaderas correspondientes a los datos de prueba para comparar el rendimiento.\n",
    "- **preprocessing_fn**: Función utilizada para preprocesar los datos de prueba sin procesar.\n",
    "    \n",
    "**Salidas:**\n",
    "- **metrics**: Un diccionario que contiene varias métricas de evaluación como precisión, puntuación F1, tiempo de evaluación, uso de memoria, uso de CPU y tamaño del pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e340916-7476-4c3a-b2b2-99da9194286f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53499ad4-3fae-47d0-9da7-1c3107e70106",
   "metadata": {},
   "source": [
    "### Preprocessing (Testing data) / Preprocesamiento (datos de prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76e2d4-4163-43f8-91f2-1993a229dd13",
   "metadata": {},
   "source": [
    "By applying the same preprocessing to both the training and testing data, we ensure consistent feature representation, which is essential for accurate predictions and prevents errors from data mismatches. This alignment improves model evaluation and generalization to new data. Additionally, combining all preprocessing steps into a single function allows the evaluation function to track execution time and resource usage, ensuring both consistency and computational efficiency across the entire pipeline.\n",
    "\n",
    " <hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "Al aplicar el mismo preprocesamiento tanto a los datos de entrenamiento como a los de prueba, garantizamos una representación coherente de las características, lo que resulta esencial para realizar predicciones precisas y evita errores derivados de la falta de coincidencia de los datos. Esta alineación mejora la evaluación del modelo y la generalización a nuevos datos. Además, la combinación de todos los pasos de preprocesamiento en una única función permite que la función de evaluación controle el tiempo de ejecución y el uso de recursos, garantizando la coherencia y la eficiencia computacional en todo el proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e34db3-d518-41e9-815a-58ae0350ba35",
   "metadata": {},
   "source": [
    "### ML method Preprocessing / Método de aprendizaje automático Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b4dc6-b80d-4b8d-8e33-88391752e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn_ML(X): # We apply the same pre-processing steps implemented in Notebook 3.\n",
    "    from skimage.color import rgb2gray\n",
    "    from skimage.transform import resize\n",
    "    \n",
    "    # Normalize the data to [0, 1]\n",
    "    X_pre = X.astype('float32') / 255.0\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    X_pre = np.array([rgb2gray(image) for image in X_pre])\n",
    "    \n",
    "    # Resize images to 64x64 pixels\n",
    "    X_pre = np.array([resize(image, (64, 64), anti_aliasing=True) for image in X_pre])\n",
    "    \n",
    "    # Flatten the images\n",
    "    num_samples = X_pre.shape[0]\n",
    "    X_pre = X_pre.reshape(num_samples, -1)\n",
    "    \n",
    "    return X_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa180d0-03ce-4097-a03c-3c5c33f9f59f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14307ec6-396a-4ec6-ad20-6ce395f4e73f",
   "metadata": {},
   "source": [
    "### ML Evaluation / Evaluación del aprendizaje automático"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e390b5-62f3-4d54-9172-49057608ca6b",
   "metadata": {},
   "source": [
    "#### Import data and ML model /  Importar datos y modelo de aprendizaje automático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0bfe1-b3dc-4895-9983-ee7269112b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# first let us load the testing data\n",
    "test_images = np.load('test_images.npy')      # Load image test data\n",
    "test_labels = np.load('test_labels.npy')      # Load label test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f3076-46a8-49ae-bb1e-236634c109d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97753c-b60d-427e-9ad8-cbd060004273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load the ml model from the 3rd notebook\n",
    "with open('sgd_model.pkl', 'rb') as file:\n",
    "    sgd_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5072ad-0d23-416d-8355-f584057d48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have:\n",
    "# - A trained model named like 'lr_model'\n",
    "# - Raw test data 'X_test_raw'\n",
    "# - True labels 'y_test'\n",
    "# - All pre-processing methods gathered in one function\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the pipeline\n",
    "metrics = evaluate_pipeline(sgd_model, test_images, test_labels, preprocessing_fn_ML)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    if key == 'evaluation_time':\n",
    "        print(f\"{key}: {value:.2f} seconds\")\n",
    "    elif key == 'pipeline_size':\n",
    "        print(f\"{key}: {value:.2f} MB\")\n",
    "    elif key == 'peak_memory_usage':\n",
    "        print(f\"{key}: {value:.2f} MB\")\n",
    "    elif key == 'average_cpu_usage':\n",
    "        print(f\"{key}: {value:.2f}%\")\n",
    "    elif key == 'confusion_matrix':\n",
    "        print(key)\n",
    "        print(value) \n",
    "    else:\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ba278-d0c4-4f1f-80a9-b7d02ff31f82",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512bccf-17e5-4399-8409-edc055906191",
   "metadata": {},
   "source": [
    "### CubeSatNet_CNN Evaluation / Evaluación  del CubeSatNet_CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2149f017-0a59-468e-bdcd-188611639bc7",
   "metadata": {},
   "source": [
    "#### preprocessing / preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70cc940-e497-4fb6-b86e-801d552a084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn_CNN(X):  # we did not use any preprocessing in notebook 4\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13bf5a4-2e2d-49da-8369-42d86f654bea",
   "metadata": {},
   "source": [
    "#### Import CNN model / Importar modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6a7ac-3fc2-4623-bb3a-809c183c0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CNN model from the 4th notebook\n",
    "with open('cnn_model.pkl', 'rb') as file:\n",
    "    cnn_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6537c79e-14cb-495c-adbf-3d6e6d86ca53",
   "metadata": {},
   "source": [
    "#### Evaluate the CNN pipeline /  Evaluar el CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc05ff-a71b-4cb4-a991-481e5083cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "test_labels = to_categorical(test_labels, num_classes=5)\n",
    "\n",
    "\n",
    "# Evaluate the pipeline\n",
    "metrics = evaluate_pipeline(cnn_model, test_images, test_labels, preprocessing_fn_CNN)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    if key == 'evaluation_time':\n",
    "        print(f\"{key}: {value:.2f} seconds\")\n",
    "    elif key == 'pipeline_size':\n",
    "        print(f\"{key}: {value:.2f} MB\")\n",
    "    elif key == 'peak_memory_usage':\n",
    "        print(f\"{key}: {value:.2f} MB\")\n",
    "    elif key == 'average_cpu_usage':\n",
    "        print(f\"{key}: {value:.2f}%\")\n",
    "    elif key == 'confusion_matrix':\n",
    "        print(key)\n",
    "        print(value) \n",
    "    else:\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaa2a30-1929-42cc-b456-d2ccbacc21a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc715b-c909-41ea-8017-f2a27650323d",
   "metadata": {},
   "source": [
    "### Comparison between the two models / Comparación entre los dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bafabda-93ba-4a02-b6ba-822b518ac6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
